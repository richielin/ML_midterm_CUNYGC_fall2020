\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{float}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\graphicspath{ {./figs/} }
\usepackage[a4paper, total={6in, 8in}]{geometry}
\setlength{\parskip}{1em}

\title{Midterm Project Report, Bayes Rules Classifier}
\author{Tao(Richie) Lin}
\date{\today}


\begin{document}
    \maketitle
    \begin{multicols}{2}
        \section{Abstract}
        This paper is an experiment report from the Machine Learning class with Professor Haralick. By researching, designing, and implementing the experiment, the goal is to understand better the mechanism, application, and limitation of the Bayes rule classifier.
        
        The experiment was divided into three major parts: 1) design a generator that creates a random multinominal one-dimensional array. The measurement of space and the class will be made from this array. 2) design a series of programs that implement the Bayes rule classifier with economic gain. 3) design an optimization process and evaluate the result.

        The result shows that the Bayes conditional probability has a strong correlation to the class having the highest probability when the data is randomly generated. The optimization has small or no impact on the scenario that the score of incorrect assignment is higher than the correct ones in the economic gain matrix. 
    \section{Introduction}
        As an extremely important component of modern statistics, Bayes Theorem has provided a powerful angle looking at the world with the conditional probability. Additional to the contribution to the statistics, it is also a widely used algorithm in the field of machine learning. In the class of Machine Learning, Professor Haralick has instructed us to accomplish a series of experiments, including understanding the theory of Bayes Theorem, design the experiment procedures, implementing the coding process, evaluating the result.

    \section{Experiment Process}
        As a person who started building skill sets from statistical programming, my primary programming language is R. when working on this project. There are a couple of built-in functions in R that provided the short cuts to accomplish the functions that might need a lot of additional steps to do, such as generating uniformly distributed arrays, vectorized computation instead of interactive computation, and Boolean masking subsetting instead of iterative comparison.

        The experiment was completed in three major steps: generating random multinominal datasets for training and testing; building a program that calculates the expectation of the economic gain based on the Bayes rule; Optimization by adding the marginal change on the correct assignment for a set number of iterations.

        As one of the characteristic logics in R programming or any statistics programming language, it would be easier to manipulate a dataset rather than building the program from zero. In such a case, I started with making the synthetic dataset generator. 

        \subsection{Building synthetic dataset generator}

        The synthetic dataset generator is designed to generate a random one-dimensional array with two core parameters: the number of possible values $Y$ (or number of total class if generating the variable for true class) and the number of observations. All of the generated arrays will have the same number of the observation after generation, to be combined into the dataset. Except one of the arrays will be reserved as the true class, the rest will be combined into a x tuple set and each of the arrays will represent one feature of the x tuple.

        \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{fig1.png}
        \caption{The synthetic dataset generator}
        \end{figure}
            
        As demonstrated in the slides of this experiment, to generate the discrete array should be starting with an uniform [0, 1] pseudo random array of size Y+1. In figure 1 above, the number of possible values of the targeted array is set as Y. We normalized the array by dividing each element by the sum of them. We fed it into a loop to calculate an array of cumulative probability. Each value in the cumulative probability array is a boundary value in $<p_0,...,P_y>$ as mentioned in the mathematical methodology earlier. Here we have:Let $p_i$ being the probability interval for a discrete value and $q_i$ and $q_i+1$ being the boundary of the interval $p_i$. Form the cumulative distribution: $<p_0,...,P_y>$, for $i \in {0,...Y} , p_i =\sum_{j=0}^i q_j$. Then we are going to generate another uniform [0, 1] pseudo random array $X = U[0,1]$ and compare each element $x \in X$ with the boundary values in the cumulative distribution $<p_0,...,P_y>$. If $x$ is $U[0, 1]$ then $Prob(p_1 < x <= p_2) = q_2$

        In the screenshot of the example above, the array that was being generated has a total number of element was set as 3. The uniform distributed array $q\_Y$ has a value of 0.9356429, 0.6021019, and 0.4127837. By dividing the sum of all of them, the normalized array has the values of 0.4357645, 0.1271983, 0.4370372. Finally, the cumulative distribution calculated from the loop is 0.4357645, 0.5629628, 1.0000000, which serves as the upper limit of each assignment window.

        The next step is to input the number of observations, size and generated a uniformly distributed array, d, with the range from 0 to 1 and total elements as size.
        Each element of d will be fed into a loop of comparison with the values $p\_Y$. In the example of the screenshot above, if the values are greater than 0 but smaller or equal to 4.357645, the label of this element will be assigned as 1; if it is great than the previous upper limit and smaller than 0.5629628, the label will be assigned as 2; same applies to the third circumstances, which the label of the element will be assigned as 3.

        When the iteration is completed, a one-dimensional array with the $Y$ classes, and the total number of the element as $size$, the probability of each label being assigned is $p\_Y$.
 
        However, as a relatively high level of programming language, R provides a vectorized computational logic, which allows users to apply the manipulation on the entire array level rather than looping through each of them. That character makes our functions hit the performance issue when the size was set more than 1 million. However, when the number of observations in the measurement space was not big enough (in my case, I would prefer to set it at 3 million), some x tuple would not have enough samples to calculate a meaningful prior conditional probability. In such consideration, I decided to go ahead and use one of the powerful preset functions in R: $sample()$. It generates a randomly distributed one-dimensional array when the given number of possible values of labels, number of observations, and the probability of each possible values. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{fig2.png}
            \caption{Generating dataset with specified number of classes and probabilities}
        \end{figure}
        In the screenshot above, I generated a measurement space with six features: x1 - x6 with the same number of observations: $ob = 3000000$ but different numbers and probability of the possible labels:
        \begin{itemize}
            \item $Y$, dependent variable, true class, with 4 classes and class probabilities as 0.25, 0.25, 0.2, 0.3;
            \item $x1$ - $x6$, dependent variables, feature or measurements of the x tuple. The number of classes is defined in the vector $x$ and probabilities as $prob$.
        \end{itemize} 
        They were combined into a data frame for the next step.

        \subsection{Calculating Bayes rule}
        From the slides, I learned to form the professor that the Bayes rules that are more generally applicable to real-world problem solving are considering the economics gains for every assignment. It was a fascinating idea to me and elevated my level of thinking in applying machine learning techniques in real-world problem-solving. 

        So in building the Bayes rules, I divided the constructions into three parts: 
        \begin{itemize}
        \item Building a function that takes in a training set: $D_{training} = \times_{n=1}^{N}L_n$ , then calculates the prior conditional probability $P(d|c)$ and the class probability $P(c)$ for each features combination $L_n$;
        \item Building a function that takes the prior conditional probability $P(d|c)$ and the class probability $P(c)$ from the last step, calculates the Bayes posterior conditional probability $P(c|d)$ for each feature combination $L_n$;
        \item Building a function that takes the input:
            \begin{itemize}
                \item prior conditional probability $P(d|c)$
                \item class probability $P(c)$
                \item Bayes posterior conditional probability $P(c|d)$
                \item economics gain matrix $e$
            \end{itemize} 
            and calculate the Discrete Bayes Rule $f_d(c)$ for each feature combination $L_n$. Then assigns the class based on the highest total economics gain $e(c^j, c^k, d)$ for each feature combination $d$ and return the expected total economics gain:
            $E[e] = \sum_{j=1}^K\sum_{k=1}^K e(c^j, c^k) \sum_{d\in{D}}P_{TA}(c^j, c^k, d)$:
        \end{itemize}

        Before building these functions, I divided the dataset into a training set and a testing set. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{fig3.png}
            \caption{Dividing into training and testing sets}
        \end{figure}
        This step, by providing the number of folds wanted, calculates the staring row and the ending row and saves it as a test set index. We can use the Boolean mask feature in R programming to subset the index's training set and testing set.

        The function that calculates the prior conditional probability is called fun\_prob() in my code. It takes a data frame with columns named Xn and Y and produces the table that contains the prior probability of each x tuple in every given y.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{fig4.png}
            \caption{Building the function the calculate the prior probability of given dataset}
        \end{figure}

        The first step is to convert the measurement space to an x tuple by combining all measurement features Xs. There is a handy function called paste(), which string all elements into one string in R. Using the apply() function to cast the paste() on all the elements in each feature, it produces the x tuple with all features combined.

        Rather than counting the number of appearances in a loop, R has a great function table(), which produces the count of each unique x tuple. When given the second element to the function, in our case, the true class label y, it can also produce the crosstab of x tuples and y. the result is shown below. The first column "y" is the true class $k$ in the equation, the second column "x tuple" represents the feature combination $d$, and the last column "cnt" is the count of the appearance of such combination in the training set. When summing the count of all four classes of y of the same x tuple, we will get the total counts observed of the measurement tuple.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{fig5.png}
            \caption{The count of appearances for each x-tuple of given classes}
        \end{figure}

        Using the count from the crosstab, the conditional probability can be easily calculated simply by dividing each true class label's total count: $P(d|c) = \frac{P(c,d)}{P(c)}$. Here is an example of the output:

        \begin{figure}[H]
            \centering  
            \includegraphics[width=\linewidth]{fig6.png}
            \caption{The calculated conditional probability and class prior probability of each x tuple}
        \end{figure}

        The first three columns are the same as the previous table, the column "prob\_d\_c" is $P(d|c)$, which was calculated from  $P(d|c) = \frac{P(c,d)}{P(c)}$, the last column "prob\_c" is the class probability $P(c)$

        This function is very useful to produce the elements in the Bayes theorem:

        $P(c_n|d)= \frac{P(d | c)P(c)}{P(d,c)} = \frac{P(d|c)P(c)}{\sum_{n=1}^k P(d|c_n)P(c_n))}$

        The numerator and denominator element in the expanded form of this conditional probability can be calculated with the function.

        Build the function to calculate the posterior conditional probability with the economic gain.

        Based on the Bayes conditional probability:

        $P(c_n|d)= \frac{P(d | c)P(c)}{P(d,c)} = \frac{P(d|c)P(c)}{\sum_{n=1}^k P(d|c_n)P(c_n))}$

        To calculate each possible assignment class's probability, we need to use the conditional probability multiply the class probability that we wanted to calculate from the training set in the numerator. The denominator would be the joint probability of all possible true class and the conditional probability of the x tuple from the testing set. 

        \begin{figure}[H]
            \centering  
            \includegraphics[width=\linewidth]{fig7.png}
            \caption{Building the function to calculate the Bayes decision rule with the given prior probability}
        \end{figure}

        In this step, we takes in the prior probabilities and conditional probabilities of each measurement tuple $P(c), P(d|c)$ from the training set, then calculate the $P(d|c_n) and P(c_n)$ from the testing set and made the assignment decision $c^k$.

        After this step, the confusion matrix $P_TA(c^j, c^k)$ of true class $c^j$ and assigned class $c^k$ can be made. Multiply the confusion matrix with the economic gain matrix $E[e]$, the expected total economic gain for each measurement tuple of each given class can be calculated:  $E[e] = \sum_{j=1}^K\sum_{k=1}^K e(c^j, c^k) \sum_{d\in{D}}P_{TA}(c^j, c^k, d)$. 

        Let the $f_d(c^k)$ be the conditional probability assigns class $c^k$ given measurement $d$, Based on the fair game assumption, we have $P_AT(c^k, c^j|d) = P_A(c^k|d)P_T(c^j|d)$. The probabilistic decision rule $P_TA(c^j, c^k,d) = P_T(c^j,d)f_d(c^k)$. Then we have the expected gain as: $E[e,f] = \sum_{d\in D}\sum_{k=1}^K f_d(c^k)[ \sum_{j=1}^K e(c^j, c^k) P_T(c_j, d)]$. 

        By comparing the the expected economics gain, eventually we will find the decision rule $f_d(c^k)$ that satisfies $E[e;f] >= E[e;g]$, $g$ being any possible decision rule. We will call the decision rule $f_d(c^k)$ the Bayes Decision rule. The expected gain $E[e;f]$ will also be recorded as a performance measurement for the optimization process.     

        When implementing in R, because the testing set is involved in this calculation step, I named it fun\_test(). The first step in this function is to calculate the conditional probability of the testing set. It can be done easily with the function we built earlier. 

        A nested loop is then used to calculate each possible class assignment's probability on each x tuples. Firstly loop through all the x tuples, subset the conditional probability from the training set to calculate the denominator as shared in the calculation in all possible, then loop through y to calculate the numerator in calculating the probability for each possible assignable class. 

        \begin{figure}[H]
            \centering 
            \includegraphics[width=\linewidth]{fig8.png}
            \caption{The Calculated class probability of x tuple "1.3.2.2.4.2."}
        \end{figure}

        The screenshot above is the calculation result of the x tuple 1.3.2.2.4.2, given the possible assignment class 1,2,3 and 4. the first column i is the index of the measurement tuple and second column j is the prior class, and the last column "PT\_C\_comma\_D" is the joint probability $P(c,d)$.
        
        It is a bad practice to name the column of the results different in each step, it is generated by the looping index of different procedures, which should be named differently.

        After stacking up the results for each possible assignment class, we multiply the array of probability to the economic gain matrix. 

        \begin{figure}[H]
            \centering 
            \includegraphics[width=\linewidth]{fig9.png}
            \caption{Calculating the total economic gain of all possible assigned classes}
        \end{figure}

        By summing up the economic gain, we chose the assignment with the highest overall economic gain. In the example shown above, each possible assignment's economic gain was calculated as 1.04, 1.66, 1.32, and 0.69. in the example above, the x tuple of 1.3.2.2.4.2 will be assigned to class 2 for having the highest economic gain.

        After calculating the economic gain for all x tuples, we store the assigned class and the true class, which has the highest $P(d|c)$ probability for the optimization process. Along with these probabilities, we also sum the total economic gain for all x tuples for the evaluation.
        The result set of the function looks like the screenshot below:
        \begin{figure}[H]
            \centering 
            \includegraphics[width=\linewidth]{fig10.png}
            \caption{The economic gain, assigned class and true class of each x tuple}
        \end{figure}
        Most of the x tuple in the example above got assigned as class 2 because the overall probability of class 2 is the highest (0.4596163 vs. 0.2251065, 0.1132197, and 0.2020574) also. In the economic gain matrix, the reward of assigning it to class 2 is also pretty high.

    \subsection{Optimization}
        The assignment could be rewarded by setting a marginal change value and adding it to the correct assignment's conditional probability, thus further optimized.
        Here is an example code of optimization In my experiment:
        \begin{figure}[H]
            \centering 
            \includegraphics[width=\linewidth]{fig11.png}
            \caption{Optimization Process }
        \end{figure}

        First, set a number of iteration. I choose 12 because it reaches the max expected gain after several rounds of experiments. 

        For the first round of the iteration, the prior conditional probability is calculated with the function built earlier.

        For each round of iteration following, the prior conditional probability is inherited from the previous round. 

        By looking at only the correct assignment from the Bayes rule assignment result, we can now target the x tuple and the correctly assigned y class. Merge the subset with the prior probability table as a filter, add the delta to the prior conditional probability, and then normalize it to make the probability of given x tuple in all y classes sum to 1. 

        By doing so, the correct assignment will be rewarded by the marginal value in each iteration. 

        Here we have completed the experiment with the Bayes rule classifier.

    \section{Experimental Result \& Conclusions}
        \begin{enumerate}
            \item When all the variables were randomly generated and uniformly distributed, the calculated posterior conditional distribution will have a strong correlation with the true class with the highest probability.
            \item Because each of the x tuples can be assigned to only one class when the dataset is randomly uniformly distributed, the maximum correct assignment rate would never reach 100%, regardless of how many rounds of optimization was used.
            \item Though the correct identification rate's peak will be reached after a few iterations of the optimization process, the expected gain's peak will continue to grow. The optimization result below was from a Bayes rule classifier with the economics gain as an identity matrix with dimensions equals the number of possible classes.
            \begin{figure}[H]
                \centering 
                \includegraphics[width=\linewidth]{fig12.png}
                \caption{Optimization Process }
            \end{figure}
        \end{enumerate}
\end{multicols}
\end{document}
